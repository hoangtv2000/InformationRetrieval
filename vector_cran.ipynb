{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gensim\n",
    "import time\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# Read Documents\n",
    "def load_data(path):\n",
    "    files = open(path, 'r', encoding='utf-8')\n",
    "    raw_docs = files.readlines()\n",
    "    docs = []\n",
    "    flag = 0\n",
    "    temp = \"\"\n",
    "    for doc in raw_docs:\n",
    "        if doc[:2] == '.I':\n",
    "            flag = 0\n",
    "            docs.append(temp)\n",
    "            temp = \"\"\n",
    "        elif doc[:2] == '.W':\n",
    "            flag = 1\n",
    "        elif flag == 1:\n",
    "            content = doc.replace('.', '').replace(\"''\", '').replace('\\n', '').lower()\n",
    "            content = gensim.utils.simple_preprocess(content)\n",
    "            content = [\" \"+non_stopword for non_stopword in content if non_stopword not in stopwords]\n",
    "            content = [stemmer.stem(word) for word in content]\n",
    "            temp +=  \" \".join(content)\n",
    "\n",
    "    docs = np.asarray(docs[1:])\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Vocab document\n",
    "def build_dictionary(lst_contents):\n",
    "    dictionary = set()\n",
    "    for content in lst_contents:\n",
    "        dictionary.update(content)\n",
    "    return dictionary\n",
    "\n",
    "# TF IDF Weighting\n",
    "def calc_tf_weighting(vocab, lst_contents):\n",
    "    TF = np.zeros((len(vocab), len(lst_contents)))\n",
    "    for index, word in enumerate(vocab):\n",
    "        for jndex, content in enumerate(lst_contents):\n",
    "            TF[index,jndex] = content.count(word)\n",
    "    return np.array(TF)\n",
    "\n",
    "def calc_idf_weighting(TF, N):\n",
    "    DF = np.sum(TF!=0, axis=1)\n",
    "    IDF = np.log(N/DF)\n",
    "    return np.array([IDF]).T\n",
    "\n",
    "# Normalize theo slide bài giảng\n",
    "def normalize_weighitng(TF,IDF):\n",
    "    norm = np.sum((TF**2) * (IDF**2) +1, axis=0)\n",
    "    W = TF*IDF / norm\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexing(TF_IDF):\n",
    "    index = []\n",
    "    if TF_IDF.shape[1] > 1:\n",
    "        for term in TF_IDF:\n",
    "            index_term = []\n",
    "            for TF in term[term > 0.]:\n",
    "                idx = np.where(term == TF)\n",
    "                index_term.append((idx[0][0], TF))\n",
    "            index.append(index_term)\n",
    "    elif TF_IDF.shape[1] == 1:\n",
    "        for TF in TF_IDF[TF_IDF > 0.]:\n",
    "            idx = np.where(TF_IDF == TF)\n",
    "            index.append((idx[0][0], TF))\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Calculate_AP(retrieval, query):\n",
    "    precision = 0\n",
    "    precision_list = []\n",
    "    recall_list = []\n",
    "\n",
    "    for i in range(len(retrieval)):\n",
    "        if retrieval[i] in query:\n",
    "            precision += 1\n",
    "            precision_list.append(float(precision)/(i+1))\n",
    "            recall_list.append(float(precision)/len(query))\n",
    "\n",
    "    temp = 0\n",
    "    for i in range(len(precision_list)-1,-1,-1):\n",
    "        if temp >= precision_list[i]:\n",
    "            precision_list[i] = temp\n",
    "        elif temp < precision_list[i]:\n",
    "            temp = precision_list[i]\n",
    "    \n",
    "    precision_list_11 = []\n",
    "    for i in range(11):\n",
    "        for j in range(len(recall_list)):\n",
    "            if recall_list[j] >= (i/10):\n",
    "                precision_list_11.append(max(precision_list[j:]))\n",
    "                break\n",
    "    if len(precision_list_11) < 11:\n",
    "        precision_list_11 += [0]*(11-len(precision_list_11))\n",
    "\n",
    "    return sum(precision_list_11)/len(precision_list_11)\n",
    "\n",
    "def mAP(rank, qrel):\n",
    "    mAP = []\n",
    "    for i in range(len(rank)):\n",
    "        ap = Calculate_AP(rank[i], qrel[i])\n",
    "        mAP.append(ap)\n",
    "    return np.mean(np.array(mAP))\n",
    "\n",
    "def cosine_similarity(x,y):\n",
    "    return np.dot(x,y) / (np.linalg.norm(x) * np.linalg.norm(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fqrel = open('cranqrel', 'r', encoding='utf-8')\n",
    "qrel = []\n",
    "for f in fqrel.readlines():\n",
    "    qrel.append(f.split()[:2])\n",
    "\n",
    "groundtruth = [[] for i in range(225)] \n",
    "for index in qrel:\n",
    "    idx = int(index[0]) - 1\n",
    "    truth = int(index[1]) - 1\n",
    "    groundtruth[idx].append(truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = load_data('cran.all.1400')\n",
    "docs = [doc.split() for doc in docs]\n",
    "queries = load_data('cran.qry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executed tf-idf time: 16.831020832061768\n",
      "(4278, 1400)\n",
      "(1400,)\n",
      "(4278,)\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "vocab = build_dictionary(docs)\n",
    "TF = calc_tf_weighting(vocab, docs)\n",
    "IDF = calc_idf_weighting(TF, len(docs))\n",
    "vector_W = normalize_weighitng(TF, IDF)\n",
    "print('Executed tf-idf time: {}'.format(time.time() - start_time))\n",
    "\n",
    "print(vector_W.shape)\n",
    "print(vector_W[0].shape)\n",
    "print(vector_W[:, 0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4278"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_doc = indexing(vector_W)\n",
    "len(index_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(32, 0.002315523664277359),\n",
       "  (33, 0.0005577226369853424),\n",
       "  (60, 0.003298172977800566),\n",
       "  (87, 0.0019176328267168077),\n",
       "  (103, 0.00207108917231462),\n",
       "  (111, 0.001247234390497613),\n",
       "  (207, 0.003075566934775168),\n",
       "  (266, 0.0035873045759648625),\n",
       "  (267, 0.0026167671804609737),\n",
       "  (268, 0.0032805904593704973),\n",
       "  (269, 0.0027680788973944437),\n",
       "  (295, 0.001552656575820492),\n",
       "  (296, 0.0020011149701120805),\n",
       "  (297, 0.0015519753522364006),\n",
       "  (298, 0.0007167751304306248),\n",
       "  (401, 0.0013319407049198397),\n",
       "  (402, 0.0006843448023632851),\n",
       "  (406, 0.0014932906950764953),\n",
       "  (407, 0.0015054682099304943),\n",
       "  (436, 0.0007481582144419605),\n",
       "  (446, 0.0006395045200627539),\n",
       "  (447, 0.0005833673635250012),\n",
       "  (448, 0.0006932317301774058),\n",
       "  (449, 0.0006989442026776246),\n",
       "  (489, 0.0007428647142480561),\n",
       "  (499, 0.0006236197935799408),\n",
       "  (530, 0.0011422163302742118),\n",
       "  (606, 0.0007784088155239551),\n",
       "  (652, 0.003220778108242892),\n",
       "  (966, 0.0006705155575999825),\n",
       "  (1180, 0.00199693259304285),\n",
       "  (1220, 0.002092291414964997),\n",
       "  (1221, 0.0020028447108326733),\n",
       "  (1235, 0.00154094801299642),\n",
       "  (1237, 0.0032379187895492274),\n",
       "  (1248, 0.0006909762325873295),\n",
       "  (1252, 0.001489443314037718),\n",
       "  (1272, 0.0019362618053609052),\n",
       "  (1281, 0.0025367839256624936)],\n",
       " [(343, 0.0007289199254834633)],\n",
       " [(646, 0.0011143001868872291),\n",
       "  (728, 0.0009101476816353366),\n",
       "  (734, 0.0011031083471069862),\n",
       "  (1192, 0.0009184079041584014),\n",
       "  (1329, 0.0006954298326025613),\n",
       "  (1342, 0.0007740380761543526)],\n",
       " [(625, 0.0012976390359385856)]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_doc[:4][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total available queries: 225\n",
      "Kết quả MAP: 0.39647188057329447\n"
     ]
    }
   ],
   "source": [
    "all_rank = []\n",
    "print('Total available queries: {}'.format(len(queries)))\n",
    "for query in queries:\n",
    "    qTF = calc_tf_weighting(vocab, [query.split()])\n",
    "    qTF_IDF = normalize_weighitng(qTF, IDF)\n",
    "\n",
    "    index_query = indexing(qTF_IDF)\n",
    "    res = []\n",
    "    for term_idx in index_query:\n",
    "        temp = []\n",
    "        idx, value = term_idx\n",
    "        chosen = index_doc[idx]\n",
    "        for x in chosen:\n",
    "            temp.append((x[0], x[1] * value))\n",
    "        res.append(temp)\n",
    "    result = {}\n",
    "    for num_doc in res:\n",
    "        for key, value in num_doc:\n",
    "            result[key] = result.get(key, 0) + value\n",
    "    result = list(result.items())\n",
    "    result = sorted(result, key=lambda tup: tup[1], reverse=True)\n",
    "    rank = np.array([idx[0] for idx in result])\n",
    "\n",
    "    all_rank.append(rank)\n",
    "all_rank = np.asarray(all_rank)\n",
    "print(f'Kết quả MAP: {mAP(all_rank, groundtruth)}')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "669c57335d814790a0a8ea20dc08c100929a5b6e7176d5937f3f567eb2cddf61"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gensim\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "def load_stop():\n",
    "    f = open('./nfcorpus/raw/stopwords.large', 'r', encoding='utf-8')\n",
    "    stopwords = [word.replace('\\n','') for word in f.readlines()]\n",
    "    return stopwords\n",
    "\n",
    "stopwords = load_stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "\n",
    "def load_data(path):\n",
    "    files = open(path, 'r', encoding='utf-8')\n",
    "    raw_docs = files.readlines()\n",
    "    raw_docs = [doc.replace('\\t', ' ').replace('\\n', '').split() for doc in raw_docs]\n",
    "    title = []\n",
    "    docs = []\n",
    "    \n",
    "    for doc in raw_docs:\n",
    "        title.append(doc[0])\n",
    "        content = \" \".join(doc[1:])\n",
    "        content = gensim.utils.simple_preprocess(content)\n",
    "        content = [non_stopword for non_stopword in content if non_stopword not in stopwords]\n",
    "        content = [stemmer.stem(word) for word in content]\n",
    "        docs.append(content)\n",
    "\n",
    "    return np.array(docs), np.array(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Vocab document\n",
    "def build_dictionary(lst_contents):\n",
    "    dictionary = set()\n",
    "    for content in lst_contents:\n",
    "        dictionary.update(content)\n",
    "    return dictionary\n",
    "\n",
    "# TF IDF Weighting\n",
    "def calc_tf_weighting(vocab, lst_contents):\n",
    "    TF = np.zeros((len(vocab), len(lst_contents)))\n",
    "    for index, word in enumerate(vocab):\n",
    "        for jndex, content in enumerate(lst_contents):\n",
    "            TF[index,jndex] = content.count(word)\n",
    "    return np.array(TF)\n",
    "\n",
    "def calc_idf_weighting(TF, N):\n",
    "    DF = np.sum(TF!=0, axis=1)\n",
    "    IDF = np.log(N/DF)\n",
    "    return np.array([IDF]).T\n",
    "\n",
    "# Normalize theo slide bài giảng\n",
    "def normalize_weighitng(TF,IDF):\n",
    "    norm = np.sum((TF**2) * (IDF**2) +1, axis=0)\n",
    "    W = TF*IDF / norm\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexing(TF_IDF):\n",
    "    index = []\n",
    "    if TF_IDF.shape[1] > 1:\n",
    "        for term in TF_IDF:\n",
    "            index_term = []\n",
    "            for TF in term[term > 0.]:\n",
    "                idx = np.where(term == TF)\n",
    "                index_term.append((idx[0][0], TF))\n",
    "            index.append(index_term)\n",
    "    elif TF_IDF.shape[1] == 1:\n",
    "        for TF in TF_IDF[TF_IDF > 0.]:\n",
    "            idx = np.where(TF_IDF == TF)\n",
    "            index.append((idx[0][0], TF))\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Calculate_AP(retrieval, query):\n",
    "    precision = 0\n",
    "    precision_list = []\n",
    "    recall_list = []\n",
    "\n",
    "    for i in range(len(retrieval)):\n",
    "        if retrieval[i] in query:\n",
    "            precision += 1\n",
    "            precision_list.append(float(precision)/(i+1))\n",
    "            recall_list.append(float(precision)/len(query))\n",
    "\n",
    "    temp = 0\n",
    "    for i in range(len(precision_list)-1,-1,-1):\n",
    "        if temp >= precision_list[i]:\n",
    "            precision_list[i] = temp\n",
    "        elif temp < precision_list[i]:\n",
    "            temp = precision_list[i]\n",
    "    \n",
    "    precision_list_11 = []\n",
    "    for i in range(11):\n",
    "        for j in range(len(recall_list)):\n",
    "            if recall_list[j] >= (i/10):\n",
    "                precision_list_11.append(max(precision_list[j:]))\n",
    "                break\n",
    "    if len(precision_list_11) < 11:\n",
    "        precision_list_11 += [0]*(11-len(precision_list_11))\n",
    "\n",
    "    return sum(precision_list_11)/len(precision_list_11)\n",
    "\n",
    "def mAP(rank, qrel_truth, qrel_truth_title, qrel_title ):\n",
    "    mAP = []\n",
    "    for i in range(len(rank)):\n",
    "        element = qrel_truth_title.index(qrel_title[i])\n",
    "        ap = Calculate_AP(rank[i], qrel_truth[element])\n",
    "        mAP.append(ap)\n",
    "    return np.mean(np.array(mAP))\n",
    "\n",
    "def cosine_similarity(x,y):\n",
    "    return np.dot(x,y) / (np.linalg.norm(x) * np.linalg.norm(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tập train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:18: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
     ]
    }
   ],
   "source": [
    "docs, docs_title = load_data('./nfcorpus/train/train.docs')\n",
    "queries, queries_title = load_data('./nfcorpus/train/train.all.queries')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fqrel = open('./nfcorpus/train/train.3-2-1.qrel', 'r', encoding='utf-8')\n",
    "qrel = []\n",
    "for f in fqrel.readlines():\n",
    "    content = f.replace('\\t', ' ').replace('\\n', '').split()\n",
    "    qrel.append([content[0], content[2]])\n",
    "\n",
    "groundtruth = [[] for i in range(len(queries))]\n",
    "groundtruth_title = []\n",
    "for i in qrel:\n",
    "    if i[0] not in groundtruth_title:\n",
    "        groundtruth_title.append(i[0])\n",
    "    element = groundtruth_title.index(i[0])\n",
    "    groundtruth[element].append(i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executed tf-idf time: 343.8844952583313\n",
      "(15681, 3612)\n",
      "(3612,)\n",
      "(15681,)\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "vocab = build_dictionary(docs)\n",
    "TF = calc_tf_weighting(vocab, docs)\n",
    "IDF = calc_idf_weighting(TF, len(docs))\n",
    "vector_W = normalize_weighitng(TF, IDF)\n",
    "print('Executed tf-idf time: {}'.format(time.time() - start_time))\n",
    "\n",
    "print(vector_W.shape)\n",
    "print(vector_W[0].shape)\n",
    "print(vector_W[:, 0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15681"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_doc = indexing(vector_W)\n",
    "len(index_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(94, 0.0002919345648662814),\n",
       "  (471, 0.0002343522637636209),\n",
       "  (713, 0.0002778638047875274),\n",
       "  (1145, 0.0003073534119215556),\n",
       "  (2061, 0.0002411644446946011),\n",
       "  (2150, 0.00020599538220453234),\n",
       "  (2646, 0.0002548669538697745),\n",
       "  (2657, 0.0002475819891792263),\n",
       "  (2868, 0.00026750050073915684),\n",
       "  (2881, 0.0003126212558441057),\n",
       "  (3319, 0.0002769286221239339),\n",
       "  (3528, 0.00027362057238604836)],\n",
       " [(265, 0.00032633510409427446)],\n",
       " [(362, 0.0004151293002980173), (2892, 0.0004033769467926772)]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_doc[:3][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here: 0 - Time: 0.0\n",
      "Here: 500 - Time: 91.38668084144592\n",
      "Here: 1000 - Time: 197.038480758667\n",
      "Here: 1500 - Time: 716.5844657421112\n",
      "Here: 2000 - Time: 1015.5506906509399\n",
      "Here: 2500 - Time: 1113.0604956150055\n",
      "Kết quả MAP: 0.21686824052779055\n"
     ]
    }
   ],
   "source": [
    "all_rank = []\n",
    "all_rank_title = []\n",
    "start_time = time.time()\n",
    "for i, query in enumerate(queries):\n",
    "    if i % 500 == 0:\n",
    "        print('Here: {} - Time: {}'.format(i, time.time() - start_time))\n",
    "    qTF = calc_tf_weighting(vocab, [query])\n",
    "    qTF_IDF = normalize_weighitng(qTF, IDF)\n",
    "\n",
    "    index_query = indexing(qTF_IDF)\n",
    "    res = []\n",
    "    for term_idx in index_query:\n",
    "        temp = []\n",
    "        idx, value = term_idx\n",
    "        chosen = index_doc[idx]\n",
    "        for x in chosen:\n",
    "            temp.append((x[0], x[1] * value))\n",
    "        res.append(temp)\n",
    "    result = {}\n",
    "    for num_doc in res:\n",
    "        for key, value in num_doc:\n",
    "            result[key] = result.get(key, 0) + value\n",
    "    result = list(result.items())\n",
    "    result = sorted(result, key=lambda tup: tup[1], reverse=True)\n",
    "    rank = np.array([idx[0] for idx in result])\n",
    "    all_rank.append(rank)\n",
    "    all_rank_title.append(docs_title[rank])\n",
    "    \n",
    "print(f'Kết quả MAP: {mAP(all_rank_title, groundtruth, groundtruth_title, queries_title)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kết quả MAP: 0.21686824052779055\n"
     ]
    }
   ],
   "source": [
    "all_rank_title = docs_title[all_rank]\n",
    "print(f'Kết quả MAP: {mAP(all_rank_title, groundtruth, groundtruth_title, queries_title)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tập dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:18: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executed tf-idf time: 129.1065649986267\n",
      "(14678, 3193)\n",
      "(3193,)\n",
      "(14678,)\n",
      "Here: 0 - Time: 0.0\n",
      "Kết quả MAP: 0.22219099104563436\n"
     ]
    }
   ],
   "source": [
    "docs, docs_title = load_data('./nfcorpus/dev/dev.docs')\n",
    "queries, queries_title = load_data('./nfcorpus/dev/dev.all.queries')\n",
    "\n",
    "fqrel = open('./nfcorpus/dev/dev.3-2-1.qrel', 'r', encoding='utf-8')\n",
    "qrel = []\n",
    "for f in fqrel.readlines():\n",
    "    content = f.replace('\\t', ' ').replace('\\n', '').split()\n",
    "    qrel.append([content[0], content[2]])\n",
    "\n",
    "groundtruth = [[] for i in range(len(queries))]\n",
    "groundtruth_title = []\n",
    "for i in qrel:\n",
    "    if i[0] not in groundtruth_title:\n",
    "        groundtruth_title.append(i[0])\n",
    "    element = groundtruth_title.index(i[0])\n",
    "    groundtruth[element].append(i[1])\n",
    "\n",
    "start_time = time.time()\n",
    "vocab = build_dictionary(docs)\n",
    "TF = calc_tf_weighting(vocab, docs)\n",
    "IDF = calc_idf_weighting(TF, len(docs))\n",
    "vector_W = normalize_weighitng(TF, IDF)\n",
    "print('Executed tf-idf time: {}'.format(time.time() - start_time))\n",
    "index_doc = indexing(vector_W)\n",
    "\n",
    "print(vector_W.shape)\n",
    "print(vector_W[0].shape)\n",
    "print(vector_W[:, 0].shape)\n",
    "\n",
    "all_rank = []\n",
    "all_rank_title = []\n",
    "start_time = time.time()\n",
    "for i, query in enumerate(queries):\n",
    "    if i % 500 == 0:\n",
    "        print('Here: {} - Time: {}'.format(i, time.time() - start_time))\n",
    "    qTF = calc_tf_weighting(vocab, [query])\n",
    "    qTF_IDF = normalize_weighitng(qTF, IDF)\n",
    "    index_query = indexing(qTF_IDF)\n",
    "    res = []\n",
    "    for term_idx in index_query:\n",
    "        temp = []\n",
    "        idx, value = term_idx\n",
    "        chosen = index_doc[idx]\n",
    "        for x in chosen:\n",
    "            temp.append((x[0], x[1] * value))\n",
    "        res.append(temp)\n",
    "    result = {}\n",
    "    for num_doc in res:\n",
    "        for key, value in num_doc:\n",
    "            result[key] = result.get(key, 0) + value\n",
    "    result = list(result.items())\n",
    "    result = sorted(result, key=lambda tup: tup[1], reverse=True)\n",
    "    rank = np.array([idx[0] for idx in result])\n",
    "    all_rank.append(rank)\n",
    "    all_rank_title.append(docs_title[rank])\n",
    "\n",
    "print(f'Kết quả MAP: {mAP(all_rank_title, groundtruth, groundtruth_title, queries_title)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kết quả MAP: 0.22219099104563436\n"
     ]
    }
   ],
   "source": [
    "print(f'Kết quả MAP: {mAP(all_rank_title, groundtruth, groundtruth_title, queries_title)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tập test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:18: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executed tf-idf time: 132.2583577632904\n",
      "(14663, 3162)\n",
      "(3162,)\n",
      "(14663,)\n",
      "Here: 0 - Time: 0.0\n",
      "Kết quả MAP: 0.2258748886006781\n"
     ]
    }
   ],
   "source": [
    "docs, docs_title = load_data('./nfcorpus/test/test.docs')\n",
    "queries, queries_title = load_data('./nfcorpus/test/test.all.queries')\n",
    "\n",
    "fqrel = open('./nfcorpus/test/test.3-2-1.qrel', 'r', encoding='utf-8')\n",
    "qrel = []\n",
    "for f in fqrel.readlines():\n",
    "    content = f.replace('\\t', ' ').replace('\\n', '').split()\n",
    "    qrel.append([content[0], content[2]])\n",
    "\n",
    "groundtruth = [[] for i in range(len(queries))]\n",
    "groundtruth_title = []\n",
    "for i in qrel:\n",
    "    if i[0] not in groundtruth_title:\n",
    "        groundtruth_title.append(i[0])\n",
    "    element = groundtruth_title.index(i[0])\n",
    "    groundtruth[element].append(i[1])\n",
    "\n",
    "start_time = time.time()\n",
    "vocab = build_dictionary(docs)\n",
    "TF = calc_tf_weighting(vocab, docs)\n",
    "IDF = calc_idf_weighting(TF, len(docs))\n",
    "vector_W = normalize_weighitng(TF, IDF)\n",
    "print('Executed tf-idf time: {}'.format(time.time() - start_time))\n",
    "\n",
    "print(vector_W.shape)\n",
    "print(vector_W[0].shape)\n",
    "print(vector_W[:, 0].shape)\n",
    "\n",
    "all_rank = []\n",
    "start_time = time.time()\n",
    "for i, query in enumerate(queries):\n",
    "    if i % 500 == 0:\n",
    "        print('Here: {} - Time: {}'.format(i, time.time() - start_time))\n",
    "    qTF = calc_tf_weighting(vocab, [query])\n",
    "    qTF_IDF = normalize_weighitng(qTF, IDF)\n",
    "    dists = cosine_similarity(qTF_IDF.T, vector_W)[0]\n",
    "    rank = np.argsort(dists)[::-1]\n",
    "    all_rank.append(rank)\n",
    "    \n",
    "all_rank_title = docs_title[all_rank]\n",
    "print(f'Kết quả MAP: {mAP(all_rank_title, groundtruth, groundtruth_title, queries_title)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kết quả MAP: 0.2258748886006781\n"
     ]
    }
   ],
   "source": [
    "print(f'Kết quả MAP: {mAP(all_rank_title, groundtruth, groundtruth_title, queries_title)}')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "669c57335d814790a0a8ea20dc08c100929a5b6e7176d5937f3f567eb2cddf61"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
